{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c442ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e19c20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd68f490",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e89173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66902395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741081b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "054dc441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/Users/awf/notebook/spark-3.3.1-bin-hadoop3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f6cc40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca3f304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "appName = \"Spark - Setting Log Level\"\n",
    "master = \"local\"\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(appName) \\\n",
    "    .master(master) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"OFF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b323e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IllegalArgumentException: requirement failed: Supplied level newLevel did not match one of: \n",
    "\n",
    "ALL,DEBUG,ERROR,FATAL,TRACE,WARN,INFO,OFF\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d72b695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://mbp-de-thomas.home:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>UDF</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcdf81cdbb0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "spark = SparkSession.builder.appName('UDF').getOrCreate()\n",
    "\n",
    "#spark.sparkContext.setLogLevel(\"OFF\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e93c5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nous créons également la variable sc qui est un parkContext issue de la variable spark*\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"OFF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b906459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import input_file_name\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "from pyspark.sql.types import StructField, StringType, IntegerType, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d00948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split\n",
    "\n",
    "@pandas_udf(\"col1 string, col2 long\")\n",
    "def func(s1: pd.Series, s2: pd.Series, s3: pd.DataFrame) -> pd.DataFrame:\n",
    "    s3['col2'] = s1 + s2.str.len()\n",
    "    return s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "755e877c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- long_col: long (nullable = true)\n",
      " |-- string_col: string (nullable = true)\n",
      " |-- struct_col: struct (nullable = true)\n",
      " |    |-- col1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark DataFrame that has three columns including a struct column.\n",
    "df = spark.createDataFrame(\n",
    "    [[1, \"a string\", (\"a nested string\",)]],\n",
    "    \"long_col long, string_col string, struct_col struct<col1:string>\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "954e0a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+\n",
      "|long_col|string_col|       struct_col|\n",
      "+--------+----------+-----------------+\n",
      "|       1|  a string|{a nested string}|\n",
      "+--------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91d05d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- func(long_col, string_col, struct_col): struct (nullable = true)\n",
      " |    |-- col1: string (nullable = true)\n",
      " |    |-- col2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(func(\"long_col\", \"string_col\", \"struct_col\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "327c5262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+\n",
      "|func(long_col, string_col, struct_col)|\n",
      "+--------------------------------------+\n",
      "|                  {a nested string, 9}|\n",
      "+--------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(func(\"long_col\", \"string_col\", \"struct_col\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67d45e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f17930fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    name|\n",
      "+--------+\n",
      "|John Doe|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split\n",
    "\n",
    "@pandas_udf(\"string\")\n",
    "def to_upper(s: pd.Series) -> pd.Series:\n",
    "    return s.str.upper()\n",
    "\n",
    "df = spark.createDataFrame([(\"John Doe\",)], (\"name\",))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "343588d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|to_upper(name)|\n",
      "+--------------+\n",
      "|      JOHN DOE|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(to_upper(\"name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf7bc95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "169f0140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "@pandas_udf(\"long\")\n",
    "def calculate(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    # Do some expensive initialization with a state\n",
    "    state = very_expensive_initialization()\n",
    "    for x in iterator:\n",
    "        # Use that state for whole iterator.\n",
    "        yield calculate_with_state(x, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5aa13813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/awf/notebook/spark-3.3.1-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Users/awf/notebook/spark-3.3.1-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "[Stage 22:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|plus_one(v)|\n",
      "+-----------+\n",
      "|          2|\n",
      "|          3|\n",
      "|          4|\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 22:===========================================>              (3 + 1) / 4]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from typing import Iterator\n",
    "@pandas_udf(\"long\")\n",
    "def plus_one(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    for s in iterator:\n",
    "        yield s + 1\n",
    "\n",
    "df = spark.createDataFrame(pd.DataFrame([1, 2, 3], columns=[\"v\"]))\n",
    "df.select(plus_one(df.v)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02d834e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|        Name|\n",
      "+-----+------------+\n",
      "|    1|  john jones|\n",
      "|    2|tracey smith|\n",
      "|    3| amy sanders|\n",
      "+-----+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+--------------+\n",
      "|Seqno|        Name|to_upper(Name)|\n",
      "+-----+------------+--------------+\n",
      "|    1|  john jones|    JOHN JONES|\n",
      "|    2|tracey smith|  TRACEY SMITH|\n",
      "|    3| amy sanders|   AMY SANDERS|\n",
      "+-----+------------+--------------+\n",
      "\n",
      "+-----+------------+------------+\n",
      "|Seqno|        Name|   upper_col|\n",
      "+-----+------------+------------+\n",
      "|    1|  john jones|  JOHN JONES|\n",
      "|    2|tracey smith|TRACEY SMITH|\n",
      "|    3| amy sanders| AMY SANDERS|\n",
      "+-----+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "#spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "# Prepare Data\n",
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.show()\n",
    "\n",
    "# imports\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StringType\n",
    "import pandas as pd\n",
    "\n",
    "# create pandas_udf\n",
    "@pandas_udf(StringType())\n",
    "def to_upper(s: pd.Series) -> pd.Series:\n",
    "    return s.str.upper()\n",
    "\n",
    "# Using UDF with select()\n",
    "df.select(\"Seqno\",\"Name\",to_upper(\"Name\")).show()\n",
    "\n",
    "# Using UDF with withColumn()\n",
    "df.withColumn(\"upper_col\",to_upper(\"Name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57d69939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09c3d526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.818666</td>\n",
       "      <td>0.513061</td>\n",
       "      <td>0.735572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.272248</td>\n",
       "      <td>0.189142</td>\n",
       "      <td>0.421213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.559945</td>\n",
       "      <td>0.864359</td>\n",
       "      <td>0.949797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.909914</td>\n",
       "      <td>0.992102</td>\n",
       "      <td>0.741554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.679805</td>\n",
       "      <td>0.968403</td>\n",
       "      <td>0.202535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0  0.818666  0.513061  0.735572\n",
       "1  0.272248  0.189142  0.421213\n",
       "2  0.559945  0.864359  0.949797\n",
       "3  0.909914  0.992102  0.741554\n",
       "4  0.679805  0.968403  0.202535"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Generate a Pandas DataFrame\n",
    "pdf = pd.DataFrame(np.random.rand(100, 3))\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6bfd0fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(0=0.8186662468647464, 1=0.5130609817604163, 2=0.7355715729334559),\n",
       " Row(0=0.2722479066492186, 1=0.1891415247185636, 2=0.4212134102765188),\n",
       " Row(0=0.5599445550106983, 1=0.8643593339869255, 2=0.9497969959680351),\n",
       " Row(0=0.9099139217992909, 1=0.9921020177135674, 2=0.7415543876581406),\n",
       " Row(0=0.6798045639833243, 1=0.9684033493858719, 2=0.20253525116862536)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Spark DataFrame from a Pandas DataFrame using Arrow\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f667b3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Spark DataFrame back to a Pandas DataFrame using Arrow\n",
    "result_pdf = df.select(\"*\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f708959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.818666</td>\n",
       "      <td>0.513061</td>\n",
       "      <td>0.735572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.272248</td>\n",
       "      <td>0.189142</td>\n",
       "      <td>0.421213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.559945</td>\n",
       "      <td>0.864359</td>\n",
       "      <td>0.949797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.909914</td>\n",
       "      <td>0.992102</td>\n",
       "      <td>0.741554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.679805</td>\n",
       "      <td>0.968403</td>\n",
       "      <td>0.202535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.357483</td>\n",
       "      <td>0.130592</td>\n",
       "      <td>0.058661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.358264</td>\n",
       "      <td>0.932145</td>\n",
       "      <td>0.234140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.678177</td>\n",
       "      <td>0.875813</td>\n",
       "      <td>0.281925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.516185</td>\n",
       "      <td>0.369003</td>\n",
       "      <td>0.524335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.042162</td>\n",
       "      <td>0.976980</td>\n",
       "      <td>0.811137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2\n",
       "0   0.818666  0.513061  0.735572\n",
       "1   0.272248  0.189142  0.421213\n",
       "2   0.559945  0.864359  0.949797\n",
       "3   0.909914  0.992102  0.741554\n",
       "4   0.679805  0.968403  0.202535\n",
       "..       ...       ...       ...\n",
       "95  0.357483  0.130592  0.058661\n",
       "96  0.358264  0.932145  0.234140\n",
       "97  0.678177  0.875813  0.281925\n",
       "98  0.516185  0.369003  0.524335\n",
       "99  0.042162  0.976980  0.811137\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c75c670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63156566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a3b1a375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# Declare the function and create the UDF\n",
    "def multiply_func(a, b):\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3f123e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiply = pandas_udf(multiply_func, returnType=LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af15751b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    4\n",
      "2    9\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# The function for a pandas_udf should be able to execute with local Pandas data\n",
    "x = pd.Series([1, 2, 3])\n",
    "print(multiply_func(x, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "250693e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/awf/notebook/spark-3.3.1-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark DataFrame, 'spark' is an existing SparkSession\n",
    "df = spark.createDataFrame(pd.DataFrame(x, columns=[\"x\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0068a995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|multiply_func(x, x)|\n",
      "+-------------------+\n",
      "|                  1|\n",
      "|                  4|\n",
      "|                  9|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Execute function as a Spark vectorized UDF\n",
    "df.select(multiply(col(\"x\"), col(\"x\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbcb53d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8d08c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "434aa55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x\n",
       "0  1\n",
       "1  2\n",
       "2  3"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf, struct, PandasUDFType\n",
    "\n",
    "pdf = pd.DataFrame([1, 2, 3], columns=[\"x\"])\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "292da398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  x|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(pdf)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "756c651e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(x=1), Row(x=2), Row(x=3)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "43e87c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c24b3721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, LongType\n",
    "\n",
    "# When the UDF is called with a single column that is not StructType,\n",
    "# the input to the underlying function is an iterator of pd.Series.\n",
    "@pandas_udf(\"long\", PandasUDFType.SCALAR_ITER)\n",
    "#@pandas_udf(LongType, PandasUDFType.SCALAR_ITER)\n",
    "#@pandas_udf(\"float\", PandasUDFType.SCALAR_ITER)\n",
    "#@pandas_udf(IntegerType(), PandasUDFType.SCALAR_ITER)\n",
    "def plus_one(batch_iter):\n",
    "    for x in batch_iter:\n",
    "        yield x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3ecddc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|plus_one(x)|\n",
      "+-----------+\n",
      "|          2|\n",
      "|          3|\n",
      "|          4|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(plus_one(col(\"x\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d3c2e6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|multiply_two_cols(x, x)|\n",
      "+-----------------------+\n",
      "|                      1|\n",
      "|                      4|\n",
      "|                      9|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# When the UDF is called with more than one columns,\n",
    "# the input to the underlying function is an iterator of pd.Series tuple.\n",
    "@pandas_udf(\"long\", PandasUDFType.SCALAR_ITER)\n",
    "def multiply_two_cols(batch_iter):\n",
    "    for a, b in batch_iter:\n",
    "        yield a * b\n",
    "\n",
    "df.select(multiply_two_cols(col(\"x\"), col(\"x\"))).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d9728e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b8713b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|  y|\n",
      "+---+\n",
      "|  1|\n",
      "|  4|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# When the UDF is called with a single column that is StructType,\n",
    "# the input to the underlying function is an iterator of pd.DataFrame.\n",
    "@pandas_udf(\"long\", PandasUDFType.SCALAR_ITER)\n",
    "def multiply_two_nested_cols(pdf_iter):\n",
    "    for pdf in pdf_iter:\n",
    "        yield pdf[\"a\"] * pdf[\"b\"]\n",
    "\n",
    "df.select(\n",
    "    multiply_two_nested_cols(\n",
    "        struct(col(\"x\").alias(\"a\"), col(\"x\").alias(\"b\"))\n",
    "    ).alias(\"y\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a12107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dabfc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3db12ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_broadcast = sc.broadcast(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "54b4565a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 10)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_broadcast.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c867ebbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#large_broadcast.value.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "76caa0d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([0, 0]).flatMap(lambda x: large_broadcast.value).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a27b1d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_broadcast.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4aeba553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 10)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_broadcast.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e4012b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe27c483",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "55a3751a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Array' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m broadcastVar \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mbroadcast(\u001b[43mArray\u001b[49m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m      2\u001b[0m broadcastVar\u001b[38;5;241m.\u001b[39mvalue\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Array' is not defined"
     ]
    }
   ],
   "source": [
    "broadcastVar = sc.broadcast(Array(0, 1, 2, 3))\n",
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645abe75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ec25dad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('James', 'Smith', 'USA', 'CA'),\n",
       " ('Michael', 'Rose', 'USA', 'NY'),\n",
       " ('Robert', 'Williams', 'USA', 'CA'),\n",
       " ('Maria', 'Jones', 'USA', 'FL')]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "  ]\n",
    "\n",
    "\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "449e987f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[250] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize(data)\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c31ddfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('James', 'Smith', 'USA', 'California'), ('Michael', 'Rose', 'USA', 'New York'), ('Robert', 'Williams', 'USA', 'California'), ('Maria', 'Jones', 'USA', 'Florida')]\n"
     ]
    }
   ],
   "source": [
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = rdd.map(lambda x: (x[0],x[1],x[2],state_convert(x[3]))).collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fc0f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0 (v3.8.0:fa919fdf25, Oct 14 2019, 10:23:27) \n[Clang 6.0 (clang-600.0.57)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
